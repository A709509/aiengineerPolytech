{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc6904b-b484-4520-a620-82d3d8764fc9",
   "metadata": {},
   "source": [
    "# Create and store features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821b843-798a-47e0-84e3-e5247047b85e",
   "metadata": {},
   "source": [
    "feature engineering can be push to the next level when feature became a real artefact, instead of getting flat parquet columns, we will query the feature store here called FEAST to get the real value of a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8135e4d0-7015-459d-ad03-139cdca06055",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/backends.py:187: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/backends.py:187: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/backends.py:187: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "#!pip install feast\n",
    "import pandas as pd\n",
    "### Feast dependancies\n",
    "from feast import (\n",
    "    FeatureStore,\n",
    "    Entity,\n",
    "    FeatureService,\n",
    "    FeatureView,\n",
    "    Field,\n",
    "    FileSource,\n",
    "    PushSource,\n",
    "    RequestSource,\n",
    "    RepoConfig\n",
    ")\n",
    "from feast.on_demand_feature_view import on_demand_feature_view\n",
    "from feast.types import Float32, Float64, Int64,UnixTimestamp\n",
    "import requests\n",
    "import json\n",
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03c2c3-81b4-4db6-9c81-3edebeda4411",
   "metadata": {},
   "source": [
    "### Level 0 : feature store without modelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49a592-6090-4db2-89d6-843db3f9e6a8",
   "metadata": {},
   "source": [
    "The feature store take its advantage of central entities definition and global reusable features, In this part, we will just adress the feature store as a column store that we will reuse for training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9185e261-369c-4813-b316-193217603f9f",
   "metadata": {},
   "source": [
    "#### Initialize a repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0f8020-c0f9-40b3-b39d-baa38bdf5f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "fs = FeatureStore(repo_path=\"./feature_repo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725432b-64d8-4e4c-904f-41afb259fd7a",
   "metadata": {},
   "source": [
    "### Ensure data requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c612cd-f94f-4971-8648-8a01c845e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read local data from local repository\n",
    "data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39323c1f-4418-4d32-b8c4-56946d9083d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "### Transform trip_start_timestamp field to a datetime field\n",
    "data[['trip_start_timestamp']] = ...\n",
    "### add an index column that will be used as a join key\n",
    "data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ad0400-e442-42fd-a3c2-a89eab78eb1d",
   "metadata": {},
   "source": [
    "#### Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561b55d0-9cee-4b66-b9d2-8c882aa915a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "### Persist locally as parquet data\n",
    "data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b719fef5-1c89-42cd-9974-7364d20d358c",
   "metadata": {},
   "source": [
    "### Create an Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001c57ce-a733-4ac0-8f90-315658fcaf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with your initials --> john-doe initials : jd\n",
    "username_initials = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d0b181-a6c3-418d-a598-19b37eb99bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Entity is a feature \"catalogue\", it can be viewed as a mapping between a business concept and some training features\n",
    "taxi_trip_entity = Entity(name=f\"taxi_trip_{username_initials}\", join_keys=[\"index\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4610b34e-07e0-4175-8636-9033310d2c8e",
   "metadata": {},
   "source": [
    "### Create a source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500c56fc-6dc6-4139-a912-2688f3366c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### File source is used when we materialize features\n",
    "## we will use our local data in parquet\n",
    "## the event timestamps column will be the only in timestamp in current data\n",
    "taxi_data_source = FileSource(\n",
    "    path=...,\n",
    "        event_timestamp_column=...\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4a7741-e581-46f7-9cd7-ad64d5512d6f",
   "metadata": {},
   "source": [
    "### Create a feature view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a4f7847-7353-4594-9df5-b033927a920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we will create a feature view where we will map our entity with the source\n",
    "### fill the schema with the dataframe schema and using feast imported dtypes\n",
    "### the source is the one we declared earlier\n",
    "taxi_trip_global_view = FeatureView(\n",
    "    name=f\"taxi_trip_global_view_{username_initials}\",\n",
    "    ttl=timedelta(seconds=31536000 * 3),\n",
    "    entities=[taxi_trip_entity],\n",
    "    schema=[\n",
    "        Field(name=\"index\", dtype=Int64),\n",
    "        ...\n",
    "        ],    \n",
    "    source=...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc75b64-b5ac-4349-aac8-58caabff06a7",
   "metadata": {},
   "source": [
    "### Apply this model to our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb14faca-56af-40e5-94e0-d9ff01f981eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/feast/infra/offline_stores/file_source.py:161: FutureWarning: 'ParquetDataset.schema' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.schema' attribute instead (which will return an Arrow schema instead of a Parquet schema).\n",
      "  schema = ParquetDataset(path).schema.to_arrow_schema()\n"
     ]
    }
   ],
   "source": [
    "### we want to apply to objects we created (entities, features views)\n",
    "fs.apply([...,...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab4808-9bbf-4e21-bac4-30b487c196fe",
   "metadata": {},
   "source": [
    "Now in the FEAST UI, we can see the model that we just created, browse the feature view to see what has been created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82650d3c-951c-40ec-a24c-98a1ce1b625f",
   "metadata": {},
   "source": [
    "![entity](images/entity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e2a23-4258-4a7e-911c-54b44d587c42",
   "metadata": {},
   "source": [
    "### Materialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2355aa2-c5d6-4447-882f-d6b01984337e",
   "metadata": {},
   "source": [
    "Now we will use our local data and feed it to the taxi_trip_{username_initials}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfc6187f-5642-41ff-b3f8-b096aae69293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m1\u001b[0m feature views to \u001b[1m\u001b[32m2023-01-10 11:12:02+00:00\u001b[0m into the \u001b[1m\u001b[32mpostgres\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mtaxi_trip_global_view_ge\u001b[0m from \u001b[1m\u001b[32m2020-01-11 11:12:02+00:00\u001b[0m to \u001b[1m\u001b[32m2023-01-10 11:12:02+00:00\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1722: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  if dataset.partitions is not None:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1750: FutureWarning: 'ParquetDataset.metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  if dataset.metadata:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1769: FutureWarning: 'ParquetDataset.schema' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.schema' attribute instead (which will return an Arrow schema instead of a Parquet schema).\n",
      "  if dataset.schema is not None:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1797: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  if len(dataset.pieces) > 1:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1809: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  for piece, fn in zip(dataset.pieces, fns):\n",
      "/opt/conda/lib/python3.8/site-packages/dask/array/percentile.py:27: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they. (Deprecated NumPy 1.22)\n",
      "  result = np.percentile(a2, q, interpolation=interpolation).astype(a.dtype)\n",
      "100000it [00:05, 19252.77it/s]                                                                      \n"
     ]
    }
   ],
   "source": [
    "fs.materialize_incremental(end_date=datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
